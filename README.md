# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset used in this project originated from the marketing campaigns of a Portuguese banking institution gathered over direct phone calls. The goal of this project is to predict whether client subscribed a term deposit (column "Y" value of "yes" or "no") using available independent variables.

The problem was solved using 2 different approaches: Logistic regression model which was optimised using HyperDrive Pipeline and with Azure AutoML.

Both methods yielded similar performance, with just over 90% accuracy. AutoML tried 50 models and the best performing one was the VotingEnsemble. 


## Scikit-learn Pipeline

The data used in the project has one dependent variable that we are trying to predict (column “Y”: client subscribed a term deposit) and the following groups of independent variables that will be used to build a model to predict value in column “Y” (as per https://archive.ics.uci.edu/ml/datasets/Bank+Marketing# ).
- Bank client data
- Last contact information of the current campaign
- Social and economic context information
- Information related to previous campaigns with the same client

Initial dataset analysis showed that it has 32,950 rows and doesn’t have any missing values. It’s worth mention, that the downloaded dataset size is smaller than the example dataset on UCI page, which has 41,188 rows. The filename in the provided azure blob location suggested, that the data has been already split into the train, test, and validate subsets and surely enough, the following locations provided missing rows of data:

https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_test.csv
https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_validate.csv
In both experiments runs (HyperDrive pipeline and AutoML) only bankmarketing_train.csv dataset was used. It was subsequently divided into train and test subsets using default 80-20 ratio.

Out of 20 independent variables, 10 turned out to be numerical and 10 categorical. Machine learning algorithms can only use numbers as the inputs, all the categorical variables were replaced by numbers in the clean_data function of the train.py script.
Additionally, column “Duration” has been removed from the dataset as it’s not known before the call is made. At the end of the call, when “Duration” is known so is the answer to question that we trying to predict. Therefore, this variable can’t be used in viable prediction model.
It the first approach, I used LogisticRegression classification algorithm from sklearn library and  2 parameters have been adjusted through HyperDrive pipeline: inverse of regularization strength (parameter C) and Maximum number of iterations taken for the solver to converge (max_iter).
The larger the regularization strength (smaller values of C parameter) the higher the penalty for increasing the magnitude of parameters. This is to prevent overfitting the model to the train data and make it more general i.e., also applicable to the unseen test data.
In the HyperDrive run, max_iter parameter was selected out of 3 values (100, 200, or 400) and the C parameter was pooled from the interval between exp(-10) to exp(10) using logunifrom distribution. Since the parameter C represents the inverse of the regularization strength, logunifrom distribution has been selected to attain distribution of regularization strength as close to uniform as possible.
For early stopping policy, I selected a BanditPolicy class. This policy compares the current training run with the best performing run and terminates it if it’s performance metric drops below calculated threshold. The parameters I used in my pipeline:
policy = BanditPolicy(slack_factor=0.1, evaluation_interval=5, delay_evaluation=10)
would cause each run to be compared with the best performing run after each 5 algorithm runs (starting after first 10 runs) and if the run’s performance drops below 90% of current best run performance, then it would get terminated.

The best run was achieved with C= 0. 4.217379487255968and max_iter=400 with reported Accuracy of 0.9065.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
